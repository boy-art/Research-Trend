<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>NRI: Small: Collaborative Research: Active Sensing for Robotic Cameramen</AwardTitle>
    <AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2017</AwardExpirationDate>
    <AwardAmount>300000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>With advances in camera technologies, and as cloud storage, network bandwidth and protocols become available, visual media are becoming ubiquitous. Video recording became de facto universal means of instruction for a wide range of applications such as physical exercise, technology, assembly, or cooking. This project addresses the scientific and technological challenges of video shooting in terms of coverage and optimal views planning while leaving high level aspects including creativity to the video editing and post-production stages. &lt;br/&gt;&lt;br/&gt;Camera placement and novel view selection challenges are modeled as optimization problems that minimize the uncertainty in the location of actors and objects, maximize coverage and effective appearance resolution, and optimize object detection for the sake of semantic annotation of the scene. New probabilistic models capture long range correlations when the trajectories of actors are only partially observable. Quality of potential novel views is modeled in terms of resolution that is optimized by maximizing the coverage of a 3D orientation histogram while an active view selection process for object detection minimizes a dynamic programming objective function capturing the loss due to classification error as well as the resources spent for each view.&lt;br/&gt;&lt;br/&gt;The project advances active sensing and perception and provides the technology for further automation on video capturing. Such technology has broader impact on the production of education videos for online courses as well as in telepresence applications. Research results are integrated into robotics and digital media programs addressing K-12 students.</AbstractNarration>
    <MinAmdLetterDate>09/12/2013</MinAmdLetterDate>
    <MaxAmdLetterDate>04/29/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1317788</AwardID>
    <Investigator>
      <FirstName>Ibrahim</FirstName>
      <LastName>Isler</LastName>
      <EmailAddress>isler@cs.umn.edu</EmailAddress>
      <StartDate>09/12/2013</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Minnesota-Twin Cities</Name>
      <CityName>Minneapolis</CityName>
      <ZipCode>554552070</ZipCode>
      <PhoneNumber>6126245599</PhoneNumber>
      <StreetAddress>200 OAK ST SE</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Minnesota</StateName>
      <StateCode>MN</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>8013</Code>
      <Text>National Robotics Initiative</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>8086</Code>
      <Text>Natl Robotics Initiative (NRI)</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9251</Code>
      <Text>RES EXPER FOR UNDERGRAD-SUPPLT</Text>
    </ProgramReference>
  </Award>
</rootTag>
