<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>SBIR Phase II: Extracting Valuable Information Automatically from Objects with Surface Impressions via Photographs and Interactive Digital Surrogates</AwardTitle>
    <AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2016</AwardExpirationDate>
    <AwardAmount>905399</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07070000</Code>
      <Directorate>
        <LongName>Directorate For Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Peter Atherton</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The innovation, CUNAT (CUNeiform Automated Translator) attacks significant problems in a discipline wedded to conservative manual methods. Ancient Mesopotamian legal treatises, Egyptian medical records, and Roman political accounts--telling us exactly what happened in the words of the very people whose actions formed our history - languish untranslated, because there are too many of them, translation takes too long, and there are too few linguistic experts. "Cuneiform text genres include everything...from mathematical and grammatical exercises, beer recipes, international treaties, musical scores, legal codes, religious rituals, sales receipts, and astronomical tables....[M]useums...have acquired approximately 400,000 tablets, with thousands being unearthed every year.... [S]cholars continue to make unique and valuable contributions to the study of history, law, religion, linguistics, mathematics, and science" (Digital Hammurabi Project). CUNAT will allow anyone to create 3D models of artifacts and extract meaning from their impressions more effectively and quickly than existing tools. Phase II innovations include: extending the limits of image-based feature detection through unique algorithms based on 3D models automatically generated from photographs; isolating meaningful characters from noise across an array of surface shapes; classifying the characters of many dialects based on geometric characteristics; and performing automatic translation using graph-based algorithms and rules engines rather than traditional word-by-word methods.&lt;br/&gt;&lt;br/&gt;The broader/commercial impact of our innovation are new learning and analysis methods: a high-school student enters an art museum and sees an object with triangular-shaped surface incisions; the label reads "cuneiform tablet, 864BCE, Nimrud;" curious, the student aims a smartphone camera at the object and clicks; the images are uploaded into the project?s software app, which generates an interactive 3D model and an English translation of the inscription. The student may become the first person to have read the text and discover previously unknown astronomical knowledge detailed by the ancient scribe. Their software gives this research power and excitement of discovery to everyone, every institution, anywhere, anytime, because neither expensive special equipment nor complex calibration or lighting are necessary to produce results with unprecedented efficiency and accuracy. The innovations will address critical needs across the field of archaeology and produce a commercially viable product with applications to problems in museum exhibition, numismatics, and digital art history. In addition, accurate processes will be widely applicable, including forensics, paleontology, auto mechanics, personalized 3D printing, and carpentry. Beyond the insight into the past that will inevitably accrue, economic and technological advantages will motivate commercial adoption within and beyond our target market.</AbstractNarration>
    <MinAmdLetterDate>08/29/2013</MinAmdLetterDate>
    <MaxAmdLetterDate>08/07/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1330139</AwardID>
    <Investigator>
      <FirstName>Donald</FirstName>
      <LastName>Sanders</LastName>
      <EmailAddress>dsanders@learningsites.com</EmailAddress>
      <StartDate>08/29/2013</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Learning Sites, Inc.</Name>
      <CityName>Williamstown</CityName>
      <ZipCode>012672232</ZipCode>
      <PhoneNumber>4134582828</PhoneNumber>
      <StreetAddress>151 Bridges Road</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Massachusetts</StateName>
      <StateCode>MA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>5373</Code>
      <Text>SMALL BUSINESS PHASE II</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>169E</Code>
      <Text>SBIR Tech Enhan Partner (TECP)</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>5373</Code>
      <Text>SMALL BUSINESS PHASE II</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7744</Code>
      <Text>RAHSS</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>8032</Code>
      <Text>Software Services and Applications</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>8039</Code>
      <Text>Information, Communication &amp; Computing</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>8043</Code>
      <Text>System Design and Simulation</Text>
    </ProgramReference>
  </Award>
</rootTag>
