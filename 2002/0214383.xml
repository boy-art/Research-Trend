<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Learning Minimal Representations for Visual Navigation and Recognition II</AwardTitle>
    <AwardEffectiveDate>08/01/2003</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2010</AwardExpirationDate>
    <AwardAmount>426799</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04040500</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Behavioral and Cognitive Sci</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Lawrence Robert Gottlob</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Consider how you find your way to the grocery store or learn the layout of a new mall, or how scientists might build a robot that can be dropped on Mars to navigate its surface. People, animals, and robots must navigate complex environments, but different strategies are applied in different situations. One may get to the grocery store by dead reckoning like ants, following landmarks like honeybees, or one can use a precise "memory map" of the environment. Moreover, clever combinations of strategies can make it easier to find the way. The present research effort specifically explores how these strategies are integrated to allow robust visual navigation.&lt;br/&gt;&lt;br/&gt;With NSF support, Dr. Michael Tarr and Dr. William Warren study how people learn the layout of new environments, the geometry of the resulting spatial knowledge, and how it is used to navigate. The uniqueness of their approach is to study actual navigation behavior, as people actively walk through a computer-generated virtual environment (the VENLab - see http://www.cog.brown.edu/Research/ven_lab/ ). Participants wear a head-mounted virtual reality display and walk freely in a 40 x 40 ft area. Their movements are recorded by a tracking system in the ceiling. After participants learn the layout, the environment can be surreptitiously changed, and they must, in effect, find a new route to the grocery store. By distorting the virtual world or changing the properties of landmarks, these scientists determine the navigational strategies people use and how they rely on routes, landmarks, and the geometry of space.</AbstractNarration>
    <MinAmdLetterDate>07/31/2003</MinAmdLetterDate>
    <MaxAmdLetterDate>12/17/2009</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0214383</AwardID>
    <Investigator>
      <FirstName>Michael</FirstName>
      <LastName>Tarr</LastName>
      <EmailAddress>michaeltarr@cmu.edu</EmailAddress>
      <StartDate>07/31/2003</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>William</FirstName>
      <LastName>Warren</LastName>
      <EmailAddress>Bill_Warren@brown.edu</EmailAddress>
      <StartDate>07/31/2003</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Brown University</Name>
      <CityName>Providence</CityName>
      <ZipCode>029129002</ZipCode>
      <PhoneNumber>4018632777</PhoneNumber>
      <StreetAddress>BOX 1929</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Rhode Island</StateName>
      <StateCode>RI</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0116000</Code>
      <Name>Human Subjects</Name>
    </FoaInformation>
    <ProgramElement>
      <Code>1320</Code>
      <Text>ECONOMICS</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>7252</Code>
      <Text>PERCEPTION, ACTION &amp; COGNITION</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>0000</Code>
      <Text>UNASSIGNED</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7266</Code>
      <Text>ENHANCING HUMAN PERFORMANCE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>OTHR</Code>
      <Text>OTHER RESEARCH OR EDUCATION</Text>
    </ProgramReference>
  </Award>
</rootTag>
