<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CAREER: Computational and Statistical Tradeoffs in Massive Data Analysis</AwardTitle>
    <AwardEffectiveDate>02/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>01/31/2019</AwardExpirationDate>
    <AwardAmount>376993</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computing and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>John Cozzens</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>In modern signal processing, one is frequently faced with statistical inference problems involving massive datasets. For example, the experiments at the Large Hadron Collider at CERN generate hundreds of petabytes of data each year, which must be stored and processed efficiently in order to further our understanding of particle physics. Similar challenges also arise in seismic monitoring where massive amounts of data are acquired over large areas via cellphone accelerometers. Analyzing such large datasets is usually viewed as a substantial computational challenge. However, if data are a signal processor?s main resource then access to more data should be viewed as an asset rather than as a burden, and larger datasets should lead to a reduction in the runtime of data analysis algorithms.&lt;br/&gt;&lt;br/&gt;This project blends concepts from computer science and from statistical signal processing to address the challenge with massive datasets by developing ?algorithm weakening? frameworks in which a data analysis procedure backs off to simpler methods as the data scale in size, leveraging the growing inferential strength of the data to ensure that a desired level of statistical accuracy is achieved with reduced runtime. The approach is concretely illustrated across a range of statistical estimation tasks, with convex relaxation techniques playing a prominent role as an algorithm weakening mechanism. In seeking a precise characterization of the computational and statistical tradeoffs obtained via convex relaxation, the investigator formalizes and studies new measures for characterizing the quality of approximation of one convex set by another. An interesting feature of this research is that convex relaxations which provide poor performance in combinatorial optimization problems may nonetheless yield useful solutions when employed in problems with inferential objectives.</AbstractNarration>
    <MinAmdLetterDate>01/22/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>07/07/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1350590</AwardID>
    <Investigator>
      <FirstName>Venkat</FirstName>
      <LastName>Chandrasekaran</LastName>
      <EmailAddress>venkatc@caltech.edu</EmailAddress>
      <StartDate>01/22/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>California Institute of Technology</Name>
      <CityName>PASADENA</CityName>
      <ZipCode>911250600</ZipCode>
      <PhoneNumber>6263956219</PhoneNumber>
      <StreetAddress>1200 E California Blvd</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7797</Code>
      <Text>COMM &amp; INFORMATION FOUNDATIONS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>1045</Code>
      <Text>CAREER: FACULTY EARLY CAR DEV</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7936</Code>
      <Text>SIGNAL PROCESSING</Text>
    </ProgramReference>
  </Award>
</rootTag>
