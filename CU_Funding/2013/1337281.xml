<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>XPS: FP: Collaborative Research: Parallel Irregular Programs: From High-Level Specifications to Run-time Optimizations</AwardTitle>
    <AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2017</AwardExpirationDate>
    <AwardAmount>375131</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computing and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Anindya Banerjee</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The high performance super-computers of today and the ordinary computers of tomorrow have an ever-increasing number of cores. Utilizing these computers efficiently will allow research advancements in every field of science, from understanding the brain to understanding the fundamental particles to understanding the cosmos. These new computers are increasingly complex and difficult to program. At the same time, standard algorithms used in science and engineering are evolving and are increasingly hard to map to these machines. Advances in programming models, tools, and implementations which make implementing complex algorithms simpler, while achieving high performance, are essential to making high performance computing a standard tool of all scientists.&lt;br/&gt;&lt;br/&gt;Regular algorithms, usually expressed with matrices, have driven high performance computing. Increasingly there is considerable interest in using large-scale computers for irregular algorithms. Irregular algorithms arise in manipulating graphs, sparse-matrices, trees, adaptive meshes, etc and are increasingly a standard tool used by computational scientists. Expressing such algorithms at a high-level has allowed high-performance run-times to achieve performance comparable to the best hand-coded implementations of these algorithms on shared-memory machines. A high level description frees the programmer from the complexities of parallel programming. The PIs are building run-times and compilers to allow the execution of complex, irregular algorithms on distributed-memory, large-scale computers. A high-level representation allows the system to exploit considerable knowledge about the semantics of the algorithm to optimize communication, mask latency, and achieve high-performance.</AbstractNarration>
    <MinAmdLetterDate>08/29/2013</MinAmdLetterDate>
    <MaxAmdLetterDate>08/29/2013</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1337281</AwardID>
    <Investigator>
      <FirstName>Keshav</FirstName>
      <LastName>Pingali</LastName>
      <EmailAddress>pingali@cs.utexas.edu</EmailAddress>
      <StartDate>08/29/2013</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Andrew</FirstName>
      <LastName>Lenharth</LastName>
      <EmailAddress>lenharth@ices.utexas.edu</EmailAddress>
      <StartDate>08/29/2013</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Texas at Austin</Name>
      <CityName>Austin</CityName>
      <ZipCode>787121532</ZipCode>
      <PhoneNumber>5124716424</PhoneNumber>
      <StreetAddress>101 E. 27th Street, Suite 5.300</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Texas</StateName>
      <StateCode>TX</StateCode>
    </Institution>
    <ProgramElement>
      <Code>8283</Code>
      <Text>Exploiting Parallel&amp;Scalabilty</Text>
    </ProgramElement>
  </Award>
</rootTag>
