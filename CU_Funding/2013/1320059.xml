<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Small: Binaural Sound Source Separation Robust to Listener Head Movements</AwardTitle>
    <AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2016</AwardExpirationDate>
    <AwardAmount>186624</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Tatiana D. Korelsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The goal of this project is to develop a new binaural model to separate sounds in complex environments. The new aspect of the model is that it can utilize head movements to improve its localization performance by analyzing dynamic localization cues and combining these with information about its own head position. In addition, the model uses a dual approach to eliminate the influence of room reflections on sound source localization and segregation. In the first stage, specular reflections are eliminated using an autocorrelation- based algorithm. In the second stage, diffuse reverberation is removed by measuring interaural cross correlation across time/frequency bins, knowing that these values decrease with decreasing direct-to- reverberant energy ratio. The model development is accompanied by a behavioral study to better understand the underlying principles of how humans can perform robustly in complex scenarios. The results are also used as a benchmark test for the model algorithms.&lt;br/&gt;&lt;br/&gt;This project intends to bridge the gap that exists between fundamentally knowing how the auditory system processes binaural tasks for simple multiple-sound-source scenarios, and understanding and modeling how it performs when the environment reaches real-life complexity. The resulting model is expected to operate in real time to localize sound sources in robot or surveillance applications or serve as a front end for sound- source separation algorithms, speech recognizers, predictors for acoustical quality of rooms, and Computational Auditory Scene Analysis (CASA) models.</AbstractNarration>
    <MinAmdLetterDate>08/06/2013</MinAmdLetterDate>
    <MaxAmdLetterDate>08/06/2013</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1320059</AwardID>
    <Investigator>
      <FirstName>Jonas</FirstName>
      <LastName>Braasch</LastName>
      <EmailAddress>braasj@rpi.edu</EmailAddress>
      <StartDate>08/06/2013</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Rensselaer Polytechnic Institute</Name>
      <CityName>Troy</CityName>
      <ZipCode>121803522</ZipCode>
      <PhoneNumber>5182766000</PhoneNumber>
      <StreetAddress>110 8TH ST</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7252</Code>
      <Text>PERCEPTION, ACTION &amp; COGNITION</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
  </Award>
</rootTag>
