<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>EAGER: Exploring the Use of Synthetic Speech as Reference Model to Detect Salient Emotional Segments in Speech</AwardTitle>
    <AwardEffectiveDate>03/15/2013</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2014</AwardExpirationDate>
    <AwardAmount>59338</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Tatiana D. Korelsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This EArly Grant for Exploratory Research aims to create neutral reference model from synthetic speech to contrast the emotional content of a speech signal. Emotional understanding is a crucial skill in human communication. For this reason, modeling and recognizing emotions is essential in the design and implementation of interfaces that are more in tune with the user's needs. Starting from the premise that paralinguistic information is non-uniformly conveyed across time, this study aims to identify emotionally prominent regions or focal points across various acoustic features. The study explores a novel approach based on synthetic speech to build reference models characterizing patterns observed in neutral speech. These reference models are used to contrast the emotional information observed in localized segments of a speech signal. The study builds a synthetic speech signal that conveys the same lexical information and is timely aligned with the target sentence in the database. Since it is expected that a single synthetic speech will not capture the full range of variability observed in neutral speech, the study explores approaches to produce different neutral synthetic realizations. After creating a parallel corpus with time-aligned synthetic speech, the study explores how well synthetic speech captures the acoustic patterns and emotional percepts of neutral, nonemotional speech. Then, a target signal from the database is compared with the properties observed across the family of synthesized signals. &lt;br/&gt;&lt;br/&gt;The study presents a novel approach to build a robust emotion recognition system that exploits the underlying nonuniform externalization process of expressive behaviors. Algorithms that able to identify localized emotional segments have the potential to shift the current approaches used in the area of affective computing. Instead of recognizing the emotional content of pre-segmented sentences, the problem is formulated as a detection paradigm, which is appealing from an application perspective. These advances represent a transformative breakthrough in the area of behavioral analysis and affective computing. The proposed models and algorithms provide numerous insights to explore and extend theories in linguistic and paralinguistic human behavior. Having established the base infrastructure for this exploratory research, several new scientific avenues will emerge that serve as truly innovative advancements that will impact applications in security and defense, next generation of advanced user interfaces, health informatics, and education. Furthermore, the scientific methods are enriching venues for interdisciplinary training and mentoring for undergraduate and graduate students.</AbstractNarration>
    <MinAmdLetterDate>03/13/2013</MinAmdLetterDate>
    <MaxAmdLetterDate>03/13/2013</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1329659</AwardID>
    <Investigator>
      <FirstName>Carlos</FirstName>
      <LastName>Busso</LastName>
      <EmailAddress>busso@utdallas.edu</EmailAddress>
      <StartDate>03/13/2013</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Texas at Dallas</Name>
      <CityName>Richardson</CityName>
      <ZipCode>750803021</ZipCode>
      <PhoneNumber>9728832313</PhoneNumber>
      <StreetAddress>800 W. Campbell Rd., AD15</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Texas</StateName>
      <StateCode>TX</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7916</Code>
      <Text>EAGER</Text>
    </ProgramReference>
  </Award>
</rootTag>
