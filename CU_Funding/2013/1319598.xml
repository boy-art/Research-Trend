<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>HCC: Small: Head Activated Technology for Off-the-Shelf Mobile Devices</AwardTitle>
    <AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2017</AwardExpirationDate>
    <AwardAmount>496020</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Ephraim P. Glinert</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Mobile computing devices offer increasingly rich human-computer interaction opportunities through the use of new sensor technology such as multi-touch surfaces, microphones, and cameras. These advances provide users with richer sets of interaction modalities and motivate new and novel human-centric interfaces. Among the new input modalities, touch-based interaction is the most advanced and widely deployed, while speech is gaining increased attention. However, even with recent breakthroughs in computer vision and depth and motion sensing technology, the imaging modality remains the least developed for mobile devices. And yet image-based sensing (a) would permit richer interaction opportunities for a general user population and (b) holds special promise for individuals with disabilities who find touch-based interfaces very challenging to use.&lt;br/&gt;&lt;br/&gt;This project focuses on the development of Head-Activated Technology for off-the- shelf Mobile Devices (HAT-MD). HAT-MD systems will employ the next-generation imaging technology that will be included in a wide array of mobile devices. Advanced algorithms will detect and track user head and facial features, and map specific movements to specific interface controls and application actions. HAT-MD is initially targeted towards individuals with physical disabilities, and will both (a) extend to mobile platforms the head control interaction techniques that are often employed on desktop computers and (b) broaden the set of head and face movements that can be employed.&lt;br/&gt;&lt;br/&gt;Project deliverables include: (a) HAT-MD Algorithms: Next-generation imaging sensors will be used for detection, tracking, and 3D reconstruction. Head position and facial features will be detected, tracked, and mapped to interface controls. Computation will be distributed between the mobile devices and cloud computing services. (b) HAT-MD Applications: Head activated interfaces will be developed to address the challenges that individuals with physical disabilities have when using contemporary mobile platforms. (c) HAT-MD Evaluations: Rigorous human subject studies will evaluate the effectiveness and ease-of-use of the general HAT-MD methodology as well as specific interface and applications that are developed. Algorithms will be benchmarked to current state-of-the-art detection, tracking, and reconstruction methods.&lt;br/&gt;&lt;br/&gt;Broader Impacts: The increased accessibility provided by the HAT-MD project will create new opportunities for people to interact with mobile devices, especially for individuals with physical disabilities who currently have limited independent control of such devices. The project will also feature direct involvement by individuals with disabilities at the research, development, and evaluation phases in order to focus the intellectual development in a truly useful direction that will complement the broader impact.</AbstractNarration>
    <MinAmdLetterDate>09/09/2013</MinAmdLetterDate>
    <MaxAmdLetterDate>09/09/2013</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1319598</AwardID>
    <Investigator>
      <FirstName>Kenneth</FirstName>
      <LastName>Barner</LastName>
      <EmailAddress>barner@eecis.udel.edu</EmailAddress>
      <StartDate>09/09/2013</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Jingyi</FirstName>
      <LastName>Yu</LastName>
      <EmailAddress>yu@cis.udel.edu</EmailAddress>
      <StartDate>09/09/2013</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Delaware</Name>
      <CityName>Newark</CityName>
      <ZipCode>197162553</ZipCode>
      <PhoneNumber>3028312136</PhoneNumber>
      <StreetAddress>210 Hullihen Hall</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Delaware</StateName>
      <StateCode>DE</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7367</Code>
      <Text>Cyber-Human Systems (CHS)</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7367</Code>
      <Text>Cyber-Human Systems</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9150</Code>
      <Text>EXP PROG TO STIM COMP RES</Text>
    </ProgramReference>
  </Award>
</rootTag>
