<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Robot developmental learning of objects, actions, and tools</AwardTitle>
    <AwardEffectiveDate>09/15/2007</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2011</AwardExpirationDate>
    <AwardAmount>449999</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Todd Leen</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Planning to achieve a goal requires knowledge of objects, actions, preconditions, and consequences. These abstract concepts are at a much higher level than the ''pixel-level'' sensory and motor interfaces between an embodied robot and the continuous world. Our goal is to show how high-level concepts of object and action can be learned autonomously from experience with low-level sensorimotor interaction. &lt;br/&gt;&lt;br/&gt;We hypothesize that these concepts are part of a larger package of foundational concepts that can be learned in approximately the following sequence: using motion to discriminate objects from background; detecting tight, reliable control loops to distinguish self from non-self objects; learning preconditions and consequences of actions applied to objects; identifying ''grasp'' actions that temporarily transform a non-self object to a self object; learning actions and effects that are achievable only with such an object (a tool!).&lt;br/&gt;&lt;br/&gt;The learning process depends on representing sensorimotor interaction with the world as a stochastic dynamical system. A ''curiosity'' drive rewards improvements in prediction reliability. Evaluation uses a simulated robot child with two arms, stereo vision, and a tray of blocks and other objects. This research will help robots learn their own high-level concepts, and could provide insights into human learning disabilities.</AbstractNarration>
    <MinAmdLetterDate>09/07/2007</MinAmdLetterDate>
    <MaxAmdLetterDate>08/29/2008</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0713150</AwardID>
    <Investigator>
      <FirstName>Benjamin</FirstName>
      <LastName>Kuipers</LastName>
      <EmailAddress>kuipers@umich.edu</EmailAddress>
      <StartDate>09/07/2007</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Texas at Austin</Name>
      <CityName>Austin</CityName>
      <ZipCode>787121532</ZipCode>
      <PhoneNumber>5124716424</PhoneNumber>
      <StreetAddress>101 E. 27th Street, Suite 5.300</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Texas</StateName>
      <StateCode>TX</StateCode>
    </Institution>
  </Award>
</rootTag>
