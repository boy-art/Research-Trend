<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>SGER: Exploiting Alternative Packagings of Source Meaning in Statistical Machine Translation</AwardTitle>
    <AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2010</AwardExpirationDate>
    <AwardAmount>160763</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Tatiana D. Korelsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>SGER: Exploiting Alternative Packagings of Source Meaning in Statistical Machine Translation&lt;br/&gt;&lt;br/&gt;Current approaches in statistical machine translation (MT) miss a key&lt;br/&gt;fact: the source language sentence is not the only way the author's meaning could have been expressed. The idea that the source sentence is just one of various ``packagings'' of underlying meaning was, of course, one familiar motivation for interlingual approaches to translation; however, interlingual semantic representations have generally been abandoned as notoriously difficult to define, and equally difficult to obtain accurately with broad coverage once defined. In this project, we are revisiting the idea of "packagings" of meaning, but exploring it in practical ways consistent with current practice in statistical MT. Unlike semantic transfer or interlingual&lt;br/&gt;approaches, we encode alternatives as source paraphrase lattices, a representation that allows us to exploit generalizations about the source language while still maintaining the surface-to-surface orientation that characterizes the statistical state of the art. Our exploratory work focuses on capturing syntactic and semantic variation using Lexicalized Well Founded Grammars (LWFG), a recent formalism that balances expressiveness with practical and provable learnability results. We are quantifying and characterizing the information available in source paraphrase lattices, assessing the value of shallow paraphrasing, and exploring the relative promise of deeper techniques for source paraphase generation using LWFG and other constraint-based grammatical frameworks. The ability to capture&lt;br/&gt;generalizations via source paraphrase may open new possibilities in the translation of minority and endangered languages, which lack training corpora on the scale necessary to support standard statistical MT techniques.</AbstractNarration>
    <MinAmdLetterDate>07/16/2008</MinAmdLetterDate>
    <MaxAmdLetterDate>08/07/2009</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0838801</AwardID>
    <Investigator>
      <FirstName>Philip</FirstName>
      <LastName>Resnik</LastName>
      <EmailAddress>resnik@umiacs.umd.edu</EmailAddress>
      <StartDate>07/16/2008</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Maryland College Park</Name>
      <CityName>COLLEGE PARK</CityName>
      <ZipCode>207425141</ZipCode>
      <PhoneNumber>3014056269</PhoneNumber>
      <StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Maryland</StateName>
      <StateCode>MD</StateCode>
    </Institution>
  </Award>
</rootTag>
