<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Understanding Multi-Modal Discourse: Cognitive Resources and Speech-Gesture Integration</AwardTitle>
    <AwardEffectiveDate>08/01/2009</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2012</AwardExpirationDate>
    <AwardAmount>300000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04040000</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Behavioral and Cognitive Sci</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Betty H. Tuller</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5.&lt;br/&gt;&lt;br/&gt;Anyone who has had a phone conversation about how to how to change the oil in a car or how to make lasagna knows the importance of explanatory gestures for communication. Multimodal discourse involves the use of both visual (gestures) and auditory (speech) information during communication. Scientists still have much to learn about how people combine information in gestures with information in speech and whether people vary in their ability to use gestural information. This project evaluates the importance of working memory for combining linguistic information in speech with gestural information. Participant's brain activity will be recorded as they watch videotapes of a person talking about concrete topics such as the shape of objects, their relative sizes, and other spatial information that is difficult to convey through speech alone. The impact of gestural information will be measured by comparing brain activity recorded when the same person watches videos in which the speaker does gesture with those in which he does not. The importance of the different working memory systems will be assessed by seeing how language comprehension suffers when participants are asked to remember irrelevant words (verbal working memory), dot patterns (visuospatial working memory), and pictures of meaningless body positions (sensorimotor working memory). The impact of differences in learning style and cognitive abilities on gesture comprehension will also be examined &lt;br/&gt;&lt;br/&gt;By discovering the relative importance of verbal, visuospatial, and sensorimotor working memory for understanding face-to-face communication, this research will aid the design of more effective teaching and training methods. The project may guide the development of teaching methods that are specially adapted for people with different learning styles and could help children and adults with communicative deficits by maximizing the effects of gestural information. The project supports an early career scientist and provides summer jobs for college students, including those from minorities under-represented in science.</AbstractNarration>
    <MinAmdLetterDate>07/30/2009</MinAmdLetterDate>
    <MaxAmdLetterDate>06/02/2010</MaxAmdLetterDate>
    <ARRAAmount>300000</ARRAAmount>
    <AwardID>0843946</AwardID>
    <Investigator>
      <FirstName>Seana</FirstName>
      <LastName>Coulson</LastName>
      <EmailAddress>scoulson@ucsd.edu</EmailAddress>
      <StartDate>07/30/2009</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of California-San Diego</Name>
      <CityName>La Jolla</CityName>
      <ZipCode>920930621</ZipCode>
      <PhoneNumber>8585344896</PhoneNumber>
      <StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0116000</Code>
      <Name>Human Subjects</Name>
    </FoaInformation>
    <ProgramElement>
      <Code>7252</Code>
      <Text>PERCEPTION, ACTION &amp; COGNITION</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>0000</Code>
      <Text>UNASSIGNED</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>6890</Code>
      <Text>RECOVERY ACT ACTION</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>OTHR</Code>
      <Text>OTHER RESEARCH OR EDUCATION</Text>
    </ProgramReference>
  </Award>
</rootTag>
