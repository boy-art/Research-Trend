<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>HCC: Small: Enabling Focus Cues in Stereoscopic Displays</AwardTitle>
    <AwardEffectiveDate>08/01/2009</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2014</AwardExpirationDate>
    <AwardAmount>497600</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Ephraim P. Glinert</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Although many different approaches to 3D displays have been explored for decades, the dominant technology in practical use has been of the stereoscopic type, which provides a sense of depth by presenting two perspective images, one for each eye, of a scene from two slightly different viewing positions. A problem inherent to this approach is the lack of correctly rendered focus cues, which stems from the fact that the pairs of stereoscopic images are typically presented on 2D flat surfaces at a fixed distance from the eye. As a result, retinal image blur does not vary with the distances from an eye fixation point to other points at different depths in the simulated scene, but remains consistent with the fixed distance of the display surface. And the eye accommodation depth tends to be at the fixed distance of the 2D display while the eyes are forced to converge at different distances to view objects at different depths. Psychophysical studies have suggested that these incorrect focus cues may contribute to the compressed depth perception and visual discomfort commonly reported when viewing stereoscopic images, which have profound implications for adopting stereoscopic displays for a wide range of applications. In this project, the PI will develop an alternative 3D display based on depth-fused multifocal plane technology that offers more accurate rendering of the focus cues than conventional stereoscopic displays. Instead of focusing on the engineering aspects of developing a better 3D display, she will pursue a human-centered approach wherein human observers participate in the design process for a multi-modal 3D display platform that allows flexible adaptation of the display along multiple display modalities with different levels of focus cue accuracy, from the basic stereoscopic display mode to advanced display modes with correct or partially correct focus cues. The PI will carry out pilot experiments to validate the functions of different display modalities and to evaluate the effects of focus cues on depth perception.&lt;br/&gt;&lt;br/&gt;Broader Impacts: The new technology will offer a display solution with higher depth perception accuracy, higher stereo acuity, faster task performance, and less eye fatigue than currently available systems. Project outcomes will furthermore provide a much-needed research tool that supports investigation under controlled conditions of the various factors potentially contributing to depth perception accuracy and visual fatigue, as well as exploration of critical health issues such as the consequences for vision development in children of protracted viewing of stereoscopic images.</AbstractNarration>
    <MinAmdLetterDate>08/06/2009</MinAmdLetterDate>
    <MaxAmdLetterDate>08/06/2009</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0915035</AwardID>
    <Investigator>
      <FirstName>Hong</FirstName>
      <LastName>Hua</LastName>
      <EmailAddress>hhua@optics.arizona.edu</EmailAddress>
      <StartDate>08/06/2009</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Arizona</Name>
      <CityName>Tucson</CityName>
      <ZipCode>857194824</ZipCode>
      <PhoneNumber>5206266000</PhoneNumber>
      <StreetAddress>888 N Euclid Ave</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Arizona</StateName>
      <StateCode>AZ</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0116000</Code>
      <Name>Human Subjects</Name>
    </FoaInformation>
    <ProgramElement>
      <Code>7367</Code>
      <Text>Cyber-Human Systems (CHS)</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7367</Code>
      <Text>Cyber-Human Systems</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9215</Code>
      <Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>HPCC</Code>
      <Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
    </ProgramReference>
  </Award>
</rootTag>
