<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Context-based Indoor Object Detection</AwardTitle>
    <AwardEffectiveDate>09/15/2009</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2013</AwardExpirationDate>
    <AwardAmount>133671</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Robust and efficient indoor object detection can help people with severe vision impairment to independently access unfamiliar indoor environments. However, most existing object detection methods are developed either for a specific type of object (e.g. face) or for general nature objects (e.g., building, sky, etc) which cannot be directly applied to indoor objects due to following challenges: 1) big inter-class variations of the object model among different indoor environments, 2) small intra-class variations of different object models, 3) less texture compared to objects in natural scene or outdoor environments, 4) only part of the object is captured due to occlusions or blind user, 5) view and scale variations of the objects caused by the position and distance change between the user and the object, and 6) lack of suitable databases.&lt;br/&gt;&lt;br/&gt;This EAGER project is to explore new methods to detect indoor objects by incorporating the context information from signs (both text and iconic) and other visual clues such as signage of bathrooms and elevator floor numbers. The research enriches the study of object detection by incorporating context information, and leads to significant improvements over existing methods. The methods developed in this project provide new strategies and technologies for the blind and visually impaired to access unfamiliar indoor environments. The research also benefits many important research areas including video surveillance, intelligent conference rooms, video indexing, and human-computer interactions.</AbstractNarration>
    <MinAmdLetterDate>09/16/2009</MinAmdLetterDate>
    <MaxAmdLetterDate>06/08/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0957016</AwardID>
    <Investigator>
      <FirstName>YingLi</FirstName>
      <LastName>Tian</LastName>
      <EmailAddress>ytian@ccny.cuny.edu</EmailAddress>
      <StartDate>09/16/2009</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>CUNY City College</Name>
      <CityName>New York</CityName>
      <ZipCode>100319101</ZipCode>
      <PhoneNumber>2126505418</PhoneNumber>
      <StreetAddress>Convent Ave at 138th St</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
  </Award>
</rootTag>
