<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>HCC:Small:Collaborative Research:Design and Evaluation of Socially Engaging Avatars</AwardTitle>
    <AwardEffectiveDate>08/01/2009</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2014</AwardExpirationDate>
    <AwardAmount>259147</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>William Bainbridge</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project will employ a cyclical two-step process to develop a computational model that embeds dynamic expression and socially engaging non-verbal gestures into talking avatars, and experimentally tests its usability within digital virtual environments involving human-digital agent interaction. Specifically, the research objectives of this project include: (1) synthesis of expressive talking faces and modeling of dynamic facial expressions, (2) synthesis of socially engaging non-verbal facial gestures, and (3) in-depth usability studies on resultant avatars. &lt;br/&gt;&lt;br/&gt;Digital immersive virtual environment technology has enormous implications for human-computer interaction. Many qualities of digital human representations, particularly those of human-appearing agents, are important for social engagement and social influence. In particular, non-verbal behaviors play a critical role. Among such behaviors, arguably the most important are facial expressions of emotion, which are critical for meaningful renderings of digital agents. To date, computational models that would permit such renderings are less than optimal. Indeed, an applicable and systematic computational model for rendering spontaneous, on-the-fly non-verbal facial gestures and integrating them with speech has not been created. &lt;br/&gt;&lt;br/&gt;The success of this proposed project will remove a major barrier to the widespread application of useful digital human representation technology for all applications in which computer-mediated communication can play a role, including commerce, education, health, engineering, and entertainment applications. In addition, it will have far-reaching scientific implications, providing a computationally tractable mechanism for embedding human qualities into computer-controlled entities that are used in other scientific and engineering fields.</AbstractNarration>
    <MinAmdLetterDate>07/07/2009</MinAmdLetterDate>
    <MaxAmdLetterDate>06/22/2010</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0914965</AwardID>
    <Investigator>
      <FirstName>Zhigang</FirstName>
      <LastName>Deng</LastName>
      <EmailAddress>zdeng4@uh.edu</EmailAddress>
      <StartDate>07/07/2009</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Houston</Name>
      <CityName>Houston</CityName>
      <ZipCode>772042015</ZipCode>
      <PhoneNumber>7137435773</PhoneNumber>
      <StreetAddress>4800 Calhoun Boulevard</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Texas</StateName>
      <StateCode>TX</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0116000</Code>
      <Name>Human Subjects</Name>
    </FoaInformation>
  </Award>
</rootTag>
