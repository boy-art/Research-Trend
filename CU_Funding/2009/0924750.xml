<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Neural Basis of the Perception of Sound Location</AwardTitle>
    <AwardEffectiveDate>10/01/2009</AwardEffectiveDate>
    <AwardExpirationDate>06/30/2015</AwardExpirationDate>
    <AwardAmount>659405</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04040000</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Behavioral and Cognitive Sci</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Alumit Ishai</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>In our daily lives, we are confronted with many sounds. Distinguishing among them--identifying them, and determining where they are coming from--is essential to many aspects of our existence from carrying on a conversation in a room full of people to avoiding cars when crossing the street. With funding from the National Science Foundation, Dr. Jennifer Groh and colleagues at Duke University are investigating how the locations of sounds are represented in the brain. Unlike the visual system, the auditory system does not appear to use maps for encoding stimulus location. Rather, emerging evidence suggests that at least the horizontal dimension of auditory space is encoded via populations of neurons that respond with monotonically increasing or decreasing responses the farther a sound is located to the right or left. In theory, neurons with this kind of response pattern encode sound location via their firing rates, which exhibit a roughly one-to-one correspondence with a particular sound location. A critical limitation of this kind of coding scheme, however, is that the representation of multiple simultaneous sound locations would appear to be impossible because neurons cannot discharge at more than one firing rate at a time. And yet, humans can perceive multiple sound locations. So how does this happen? This project tests several competing hypotheses for how neurons might respond when more than one sound is present. &lt;br/&gt;&lt;br/&gt;The answers to these questions will yield important new insights into how the neural representation of sound location subserves this critical component of auditory perception. The findings generated by this research will be informative not only in the auditory domain, but will also help clarify the kinds of computational strategies the brain may employ more generally to compress information in the face of limited neural resources. The funding from this project will be used to support a research group at Duke, providing training opportunities for undergraduate, graduate, and post-doctoral trainees in cognitive and computational neuroscience.</AbstractNarration>
    <MinAmdLetterDate>09/21/2009</MinAmdLetterDate>
    <MaxAmdLetterDate>07/11/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0924750</AwardID>
    <Investigator>
      <FirstName>Jennifer</FirstName>
      <LastName>Groh</LastName>
      <EmailAddress>jmgroh@duke.edu</EmailAddress>
      <StartDate>09/21/2009</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Duke University</Name>
      <CityName>Durham</CityName>
      <ZipCode>277054010</ZipCode>
      <PhoneNumber>9196843030</PhoneNumber>
      <StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>North Carolina</StateName>
      <StateCode>NC</StateCode>
    </Institution>
  </Award>
</rootTag>
