<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CRII: RI: Learning to Predict Temporal Interestingness for Videos</AwardTitle>
    <AwardEffectiveDate>04/01/2016</AwardEffectiveDate>
    <AwardExpirationDate>03/31/2018</AwardExpirationDate>
    <AwardAmount>174674</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project examines the role that implicit feedback from viewers can play in learning a temporal interestingness function for videos. The key insight is that by leveraging pupil dilation as ground truth, supervised machine learning approaches can be applied to this problem. The ubiquitous presence of cameras in every phone, and the ability to share content with the entire world have made videos a powerful tool in the hands of everyday people. This project is to address the challenge that viewers have ever-shortening attention spans, and constructing a succinct message is hard. The project provides research and training opportunities for both undergraduate and graduate students in computer vision, machine learning, and human-centered computing.&lt;br/&gt;&lt;br/&gt;This project collects a corpus of eye-tracking data as viewers watch a collection of videos, via an off-the-shelf eye-tracking device with the objective of investigating the effectiveness of a controlled brightness calibration method to separate pupillary light reflex from pupillary emotional response. The emotional response data is leveraged as dense labels for a supervised learning approach towards predicting a video interestingness function, and algorithms are developed to cut videos to their most succinct portions based on this interestingness function. A predictive model of video interestingness could potentially impact video retrieval, summarization, and search. This would impact fields as diverse as communication and online education. Further, just as research in image saliency was hugely accelerated by the use of eye-tracking data for training, validation, and benchmarking, this project can lead to a similar unification of diverse approaches, and efforts, around an implicit, temporally dense source of ground truth for temporal interestingness in videos.</AbstractNarration>
    <MinAmdLetterDate>04/14/2016</MinAmdLetterDate>
    <MaxAmdLetterDate>04/14/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1566481</AwardID>
    <Investigator>
      <FirstName>Eakta</FirstName>
      <LastName>Jain</LastName>
      <EmailAddress>ejain@cise.ufl.edu</EmailAddress>
      <StartDate>04/14/2016</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Florida</Name>
      <CityName>GAINESVILLE</CityName>
      <ZipCode>326112002</ZipCode>
      <PhoneNumber>3523923516</PhoneNumber>
      <StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Florida</StateName>
      <StateCode>FL</StateCode>
    </Institution>
    <ProgramElement>
      <Code>026y</Code>
      <Text/>
    </ProgramElement>
    <ProgramReference>
      <Code>7367</Code>
      <Text>Cyber-Human Systems</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>8228</Code>
      <Text>CISE Resrch Initiatn Initiatve</Text>
    </ProgramReference>
  </Award>
</rootTag>
