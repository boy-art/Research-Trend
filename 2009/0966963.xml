<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Intent Seeking Algorithms for New Human-Machine Interface</AwardTitle>
    <AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2015</AwardExpirationDate>
    <AwardAmount>283362</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07020000</Code>
      <Directorate>
        <LongName>Directorate For Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Chem, Bioeng, Env, &amp; Transp Sys</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>alexander leonessa</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>PI: Joshi, Sanjay S.&lt;br/&gt;Proposal Number: 0966963&lt;br/&gt;&lt;br/&gt;Project Summary: We have created a novel human-machine interface (HMI) technology for paralyzed persons which uses the surface electromyography (sEMG) signal of a single, facial muscle for simultaneous multidimensional control of external devices. Our new controller has the potential to significantly increase the quality of life for its users. Unlike many existing humancomputer interfaces for this population, our interface is: unobtrusive &amp; inconspicuous, noninterfering with eyes/mouth/tongue, continuously available when needed, multifunctional, easy-to-use in almost any head position, and portable. We have recently discovered that humans can learn how to simultaneously manipulate power levels in two separate frequency-bands of a sEMG power spectrum (simply by contracting the muscle). Each frequency band becomes a separate control channel, which can simultaneously control different aspects of a device. Thus, we may exploit a single muscle?s natural electrical signals in far more complex ways than previously known. Using this underlying discovery, we have developed a new user interface that relies on a single head muscle?s surface EMG signal, which is easy to obtain and restricted to a small non-descript area near the ear. Our system is somewhat similar to some electroencephalographic (EEG) based brain-computer interfaces (BCI) in which a person learns to ?guide? a cursor to certain positions on a computer screen. These positions on the screen could be virtual buttons that open computer applications (human-computer interfaces), turn on/off lights (environmental control units), or control wheelchairs (mobility applications). Two central challenges in all ?cursor-guided? HMI systems are 1) intent (how does the computer know where the user intended to place the cursor?), and 2) speed at which the cursor can achieve the intended position. These two questions are intertwined in that earlier knowledge of intent can lead to faster systems. We propose to develop new ?intent-seeking? algorithms that could make our HMI much faster than our currently instantiated system. In addition, in order to conduct evaluation studies on subjects with disabilities who cannot leave either home or hospital, we will develop a new smaller mobile version of our hardware that is very easy to transport, setup, and use anywhere.&lt;br/&gt;Intellectual Merit: The use of a single sEMG signal for simultaneous multidimensional control in human-computer interfaces is potentially transformative. The notion of predicting the future location of a target (in this case a computer cursor) arises in many different applications (e.g. aerospace engineering, robotics, brain-computer interfaces). These applications employ a combination of mathematical and computer-science techniques including statistical decision making, optimal filtering, and artificial intelligence. We intend to draw from these fields to develop accurate, fast algorithms for our human-computer interface application. From a hardware perspective, entire new classes of computing devices are appearing that can perform complex computations and run graphics-intensive applications from a hand-held (or smaller) footprint.&lt;br/&gt;Designing our interface around these operating platforms will advance the area of highly portable and easy-to-use assistive interfaces.&lt;br/&gt;Broader Impact: A recent study initiated by the Reeve Foundation (2009) estimates that more than 5.5 million people live with paralysis in the United States. Many of the most severely paralyzed use ventilators to breathe, and are confined to certain head/body positions at different times during the day. Our goal is for severely paralyzed persons to regain some control of their surroundings and some basic independence. We are committed to including disabled persons in our research, not only as subjects but also as researchers themselves. As such, our work will create an additional broader impact in terms of research inclusiveness. In terms of intellectual broader impact, our new intent-seeking algorithms could have applications for many computer operating systems/programs for which disabled or non-disabled persons use various devices to guide cursors on a screen.</AbstractNarration>
    <MinAmdLetterDate>07/27/2010</MinAmdLetterDate>
    <MaxAmdLetterDate>07/27/2010</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0966963</AwardID>
    <Investigator>
      <FirstName>Sanjay</FirstName>
      <LastName>Joshi</LastName>
      <EmailAddress>maejoshi@ucdavis.edu</EmailAddress>
      <StartDate>07/27/2010</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of California-Davis</Name>
      <CityName>Davis</CityName>
      <ZipCode>956186134</ZipCode>
      <PhoneNumber>5307547700</PhoneNumber>
      <StreetAddress>OR/Sponsored Programs</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>5342</Code>
      <Text>Gen &amp; Age Rel Disabilities Eng</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>010E</Code>
      <Text>DISABILITY RES &amp; HOMECARE TECH</Text>
    </ProgramReference>
  </Award>
</rootTag>
