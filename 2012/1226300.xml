<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Doctoral Dissertation Research: The role of visual cues in imitating a new sound</AwardTitle>
    <AwardEffectiveDate>09/15/2012</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2014</AwardExpirationDate>
    <AwardAmount>8806</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04040000</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Behavioral and Cognitive Sci</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>William J. Badecker</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>How does a person learn to produce a new sound in a second language? The proposed study focuses on one possible mechanism by which a second-language learner might learn to produce a new sound: implicit imitation. Specifically, this study seeks to understand whether visual cues might facilitate implicit imitation of new sounds and thus help with developing speech production in a second language. Previous research shows that during a conversation, adults automatically and quickly converge with their interlocutors to sound more like them. In fact, this convergence occurs even when see, but do not hear the interlocutor. Not only does visual speech elicit imitation on its own, but imitation is greater for audiovisual speech than for auditory-only speech. The proposed set of experiments will examine how closely children and adults imitate the productions of unfamiliar sounds, both gesturally and acoustically, to determine whether they derive any benefit from audiovisual versus auditory exposure to the same sounds. The final component of this study will be to determine how different speaking registers can aid in imitation. The results of these experiments should add to an understanding of how the visual modality is relevant to second language learning, as well as determine the extent to which children rely on visual cues in speech processing.</AbstractNarration>
    <MinAmdLetterDate>09/19/2012</MinAmdLetterDate>
    <MaxAmdLetterDate>09/19/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1226300</AwardID>
    <Investigator>
      <FirstName>Nancy</FirstName>
      <LastName>Ward</LastName>
      <EmailAddress>nancyward@ucla.edu</EmailAddress>
      <StartDate>09/19/2012</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Megha</FirstName>
      <LastName>Sundara</LastName>
      <EmailAddress>megha.sundara@humnet.ucla.edu</EmailAddress>
      <StartDate>09/19/2012</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of California-Los Angeles</Name>
      <CityName>LOS ANGELES</CityName>
      <ZipCode>900951406</ZipCode>
      <PhoneNumber>3107940102</PhoneNumber>
      <StreetAddress>10889 Wilshire Boulevard</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1311</Code>
      <Text>LINGUISTICS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>1311</Code>
      <Text>LINGUISTICS</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9179</Code>
      <Text>GRADUATE INVOLVEMENT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>SMET</Code>
      <Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
    </ProgramReference>
  </Award>
</rootTag>
